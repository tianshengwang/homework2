---
title: "Homework 2"
subtitle: "BIOS 635"
author: "Tian Wang"
date: "1/28/2021"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, include=TRUE,
                      fig.width = 10, fig.height = 5)
```

```{r packages, echo=TRUE}
library(tidyverse)
library(broom)
library(gtsummary)
library(flextable)
library(gt)
library(caret)
library(GGally)
```

# Introduction
In this assignment you will practice using some basic machine learning methods and concepts discussed so far in lecture to develop prediction algorithms from real-world public health datasets.  You will predict a continuous outcome first, followed by a binary outcome ("Yes" or "No), using K-nearest neighbor, linear regression, and logistic regression

# 1
## Setup
In the first part, you will work with cancer mortality data in the United States at the county level from 2010-2016, with demographic information on the counties from 2013 US Census estimates.

The outcome of interest in the data is mean yearly per capita (100,000 people) cancer mortalities from 2010-2016, denoted `TARGET_deathRate` in the dataset (`cancer_reg.csv`).  So more info on the dataset in the docs folder.  

## A
First, let's look at summary statistics of the variables of interest in the data using the function `tbl_summary` in the `gtsummary` package.  Be sure to print the table as a `flextable` using the function `as_flex_table`.  Specifically:

- First, create variable `deathrate_vs_median` in dataset after reading in CSV
  - `deathrate_vs_median`="No" if `TARGET_deathRate`< `median(TARGET_deathRate)`
  - ="Yes" otherwise
- Provide stats for the following variables:
  -`TARGET_deathRate`, `medIncome`, `povertyPercent`, `MedianAge`, `PctPrivateCoverage`, `PctPublicCoverage`, `PctWhite`, `PctBlack`, `PctAsian`, `PctOtherRace`
  - **NOTE**: Don't remove variables from dataset to only those marked above.  Only use functions in `gtsummary` to remove variables from table (see `include` argument)
  - Group the summary statistics by `deathrate_vs_median`
  - Include sample size $N$ using `add_n`
  - Add p-values from one-way ANOVA test for differences in variables between "No" and "Yes" groups of `TARGET_deathRate`
  - For all variables, provide mean and standard deviation (SD) as statistics
  - Add a gray background to the cells in the row corresponding to `TARGET_deathRate`
    - **Hint**: Look at changing row/column background color in `flextable` package after using
    `as_flex_table` function
  - Also, bold text in header row after using `as_flex_table`

```{r 1a}
setwd("~/Documents/OneDrive - University of North Carolina at Chapel Hill/EPI PhD UNC/BIOS635 INTRODUCTION TO MACHINE LEARNING/HW2")
data_dir = paste0(getwd(), "/data")
cancer_reg = read.csv(paste0(data_dir, "/cancer_reg.csv") , stringsAsFactors = F) %>%
mutate(deathrate_vs_median=ifelse(TARGET_deathRate< median(TARGET_deathRate),"No","Yes")) %>% 
                       select(deathrate_vs_median, TARGET_deathRate, medIncome, povertyPercent, MedianAge, PctPrivateCoverage, PctPublicCoverage,                                   PctWhite, PctBlack, PctAsian, PctOtherRace)


 

gtsummary::tbl_summary(data=cancer_reg,              
                       by=deathrate_vs_median, 
                       statistic = list(c( TARGET_deathRate, medIncome, povertyPercent, MedianAge, PctPrivateCoverage, PctPublicCoverage, PctWhite, PctBlack, PctAsian, PctOtherRace) ~ "{mean} ({sd})")) %>%
  add_n() %>% 
  add_p() %>%
  modify_header(label = "Variable") %>%
  bold_levels() %>% as_flex_table() 
  
```

## B
Now, let's do some data visualization.  

Let's look at some 2-dimensional scatterplots of some of the above variables to assess correlation.  Specifically, recreate the following matrix of scatterplots:

- Look at the following variables
  - Use `ggpairs` from the `GGally` package:     
    - https://www.r-graph-gallery.com/199-correlation-matrix-with-ggally.html
  - `medIncome`, `povertyPercent`, `PctPrivateCoverage`, `PctPublicCoverage`
  - Color points by `deathrate_vs_median`
  - Provide some interpretation of the relationships you see in the figure.  Specifically:
    - Are there variables that have high correlations?
      - Do these high correlations make sense conceptually?
    - Compare the distributions of the variables between the two mortality rate groups (see diagonal).

```{r 1b}
GGally::ggpairs(cancer_reg,
                column = c("medIncome", "povertyPercent", "PctPrivateCoverage", "PctPublicCoverage"),
                ggplot2::aes(colour=deathrate_vs_median))
```
Yes, variables have correlations, the absolute Corr values ranged from 0.651 to 0.823.
The distributions of the variables between the two mortality rate groups are similar (density plot well overlapped).

## C
Now, let's begin to create our prediction algorithms for `TARGET_deathRate`.  First, we will start with using K-nearest neighbor (KNN).

Let's consider the features included in our summary statistics table (`TARGET_deathRate`, `medIncome`, `povertyPercent`, `MedianAge`, `PctPrivateCoverage`, `PctPublicCoverage`, `PctWhite`, `PctBlack`, `PctAsian`, `PctOtherRace`).  

- First, we will split our data into separate training and testing sets (60% in training and 40% in testing) randomly.  
- Next, train a KNN algorithm on the training dataset.
  - Use `train` function in `caret` function (see lecture slides).  Use `tuneLength`=20 and center and scale the features (see `preProcess` argument).
  - Leave everything else at default.  What is the "best" tuning parameter value chosen for parameter $k$?
  What criteria is used by R to select this "best" parameter?
  - Plot the RMSE for each considered value of $k$ during the tuning process.  What does $k$ represent based on the plot (**Hint**: see lecture slides and x-axis of plot)
- Lastly, test your algorthim at this "best" tuning parameter value on the test set.  Print out the test set performance based on RMSE, $R^2$, and MAE using `flextable`

```{r 1c}
set.seed(12) # Setting seed for reproducibility
cancer_reg_tt_index <- createDataPartition(cancer_reg$TARGET_deathRate, p=0.6, list = FALSE)
cancer_reg_train    <- cancer_reg[cancer_reg_tt_index,]
nrow(cancer_reg_train)
cancer_reg_test     <- cancer_reg[-cancer_reg_tt_index,]
nrow(cancer_reg_test)
knnFit <- caret::train(TARGET_deathRate ~ ., data = cancer_reg_train, method = "knn", tuneLength = 20)

knnFit

plot(knnFit)

apply(matrix(predict(knnFit , newdata = cancer_reg_test)), 
      2, 
      postResample, 
      obs = cancer_reg_test$TARGET_deathRate)
```
The “best” tuning parameter value chosen for parameter k = 43.
Root Mean Square Error (RMSE) was used as a riteria R to select this “best” parameter.
In the plot, k present the Neighbors associated with the minimum RMSE value.

## D
### I
Let's next move to a linear regression model for prediction.  We consider the same features listed in 1c with the same outcome variable.

- Use the same training and testing sets created in 1c
- Train a linear regression algorithm with all of the above features.  Print out the following results:
  - Coefficient estimate table from `summary` function (estimate, standard error, test statistic, p-value)
    - Create this table using the `tidy` function from `broom` and print out using `flextable`
  - Evaluate the following assumptions using the corresponding visual
    - 1. Homoskedasicity (fitted value by residual plot)
    - 2. Normality (QQ plot of residuals vs theoretical normal distribution)
  - One may argue that normally distributed residuals are not a concern for this dataset.  Why?
  - One common belief in regression is that your **outcome** is assumed to be normally distributed.  Why is
  this incorrect?
```{r 1d I}
set.seed(12) # Setting seed for reproducibility
lm_fit <- lm(formula = TARGET_deathRate ~., 
             data=cancer_reg)
#summary table of estimate, standard error, test statistic, and p-value
tidy(lm_fit) %>%
  mutate(p.value=ifelse(p.value<0.005, "<0.005", as.character(round(p.value, 2))),
         estimate=as.character(round(estimate, 3)),
         std.error=as.character(round(std.error, 3)),
         statistic=as.character(round(statistic, 3)),
         term=fct_recode(factor(term),
                        "Intercept"="(Intercept)",
                        "deathrate_vs_medianYes" = "if death rate < median death rate", 
                        "medIncome"="medIncome", 
                        "povertyPercent"="povertyPercent", 
                        "MedianAge" = "MedianAge", 
                        "PctPrivateCoverage" = "PctPrivateCoverage", 
                        "PctPublicCoverage" = "PctPublicCoverage", 
                        "PctWhite" = "PctWhite", 
                        "PctBlack" = "PctBlack",  
                        "PctAsian" = "PctAsian", 
                        "PctOtherRace" = "PctOtherRace")) %>%
  flextable()%>%
set_header_labels("term"="Variable",
"estimate"="Estimate",
"std.error"="Std. Error",
"statistic"="Z Statistic",
"p.value"="P-value") %>%
autofit()


plot(lm_fit)

```
The constant variance in fitted value by residual plot suggests Homoskedasicity.
The QQ plot of residuals vs theoretical is a near straight line, suggesting normal distribution.
One may argue that normally distributed residuals are not a concern for this dataset. Because the sample size is large (3047), which meets Large sample approximation.
One common belief in regression is that your outcome is assumed to be normally distributed. This is correct as we assume 1) Independent residuals and 2) Normally distributed residuals or Large sample approximation.


### II
- Test the algorithm developed in the previous step on the test dataset.  Print out the following in a `flextable`
  - Test set RMSE, $R^2$, adjusted $R^2$, and MAE
- In a separate `flextable`, print out these same metrics based on the performance in the training set
  - Evaluate the differences between the training and testing performance
- Based on your plots in 1b, do you have any concerns about collinearity?  If so, how would you change the set of feature variable used to fix this concern?  How did you choose this set?
  - **Note**: you don't need to actually re-run the regression analysis with this reduced set of features
```{r 1d II}
apply(matrix(predict(lm_fit, newdata = cancer_reg_test)), 
      2, 
      postResample, 
      obs = cancer_reg_test$TARGET_deathRate) 

apply(matrix(predict(lm_fit, newdata = cancer_reg_train)), 
      2, 
      postResample, 
      obs = cancer_reg_train$TARGET_deathRate) 

mmatrix <- model.matrix(TARGET_deathRate~(medIncome + povertyPercent + PctPrivateCoverage + PctPublicCoverage)^3,cancer_reg_train)

colnames(mmatrix)
library(glmnet)
cvfit <- cv.glmnet(x=mmatrix,
                     y=cancer_reg_train$TARGET_deathRate,
                     type.measure="auc",
                     family='gaussian',
                     alpha=1 #lasso penalty
)

coef(cvfit, s = "lambda.min")

plot(cvfit)

cvfit$lambda.min

cvfit$lambda.1se

lassofit <- glmnet(x=mmatrix,
                   y=cancer_reg_train$TARGET_deathRate,
                   family='gaussian',
                   alpha=1#lasso penalty
)

par(mar=c(5,5,1,13))
plot(lassofit)
vn <- colnames(mmatrix)
vnat=coef(lassofit)
vnat=vnat[-1,ncol(vnat)] # remove the intercept,
#and get the coefficients at the end of the path
axis(4, at=vnat,line=-.5,label=vn,
       las=1,tick=FALSE, cex.axis=0.8)

# 2-dimensional scatterplots of new selected variables to assess correlation
GGally::ggpairs(cancer_reg,
                column = c("povertyPercent", "PctPrivateCoverage", "PctPublicCoverage"),
                ggplot2::aes(colour=deathrate_vs_median))
```

My plots in 1b suggest collinearity, I could use variable selection strategy, e.g. LASSO, to pick important predictors. The LASSO coefficient figures shows the best 3 predictors are PctPrivateCoverage, PctPbulicCoverage, povertyPercent, thus I will drop medIncome. However, the scatterplots of new set of predictors still show collinearity, I'm not sure what to do next.



# 2
## Setup
In the second part, you will work with diabetes incidence data in the US, composed of Native American, female hospital patients at 21 years old.

The outcome of interest, `Outcome` in the data is binary indicator if the patient has a diagnosis of diabetes (0 = "No", 1 = "Yes").  You will try to predict this outcome based on patient traits as features.  See the docs folder for more information.  The dataset is called `diabetes_data.csv`.

## A
First, let's look at summary statistics of the variables of interest in the data using the function `tbl_summary` in the `gtsummary` package.  Be sure to print the table as a `flextable` using the function `as_flex_table`.  Specifically:

- Provide stats for the following variables:
  - `Pregnancies`, `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`, `Age`
  - **NOTE**: Don't remove variables from dataset to only those marked above.  Only use functions in `gtsummary` to remove variables from table (see `include` argument)
  - Group the summary statistics by `Outcome`
  - Include sample size $N$ using `add_n`
  - Add p-values from one-way ANOVA test for differences in variables between groups of `Outcome`
  - For all variables, provide mean and standard deviation (SD) as statistics
  - Also, bold text in header row after using `as_flex_table`

```{r 2a}
diabetes <- read.csv(paste0(data_dir, "/diabetes.csv") , stringsAsFactors = F) %>%
select(Outcome, Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, Age)

par(mfrow=c(1,1))
gtsummary::tbl_summary(data=diabetes,              
                       by=Outcome, 
                       statistic = list(c( Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, Age) ~ "{mean} ({sd})")) %>%
  add_n() %>% 
  add_p() %>%
  modify_header(label = "Variable") %>%
  bold_levels() %>% as_flex_table() 
```

## B
Now, let's begin to create our prediction algorithms for `Outcome`.  First, we will start with using K-nearest neighbor (KNN).

Let's consider the features included in our summary statistics table (`Pregnancies`, `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`, `Age`).  

- First, we will split our data into separate training and testing sets (60% in training and 40% in testing) randomly.  
- Next, train a KNN algorithm on the training dataset.
  - Use `train` function in `caret` function (see lecture slides).  Use `tuneLength`=20 and center and scale the features (see `preProcess` argument).
  - Leave everything else at default.  What is the "best" tuning parameter value chosen for parameter $k$?
  What criteria is used by R to select this "best" parameter?
  - Plot the Prediction Accuracy for each considered value of $k$ during the tuning process.  What does $k$ represent based on the plot (**Hint**: see lecture slides and x-axis of plot)
- Lastly, test your algorithm at this "best" tuning parameter value on the test set.  Print out the test set performance based on Prediction Accuracy, Sensitivity, Specificity, PPV, and NPV using `flextable`.
  - **Hint**: Use `confusionMatrix` function in `caret` package.  Then convert to data frame to print as
  `flextable`

```{r 2b}
set.seed(12) # Setting seed for reproducibility
diabetes_tt_index <- createDataPartition(diabetes$Outcome, p=0.6, list = FALSE)
diabetes_train    <- diabetes[diabetes_tt_index,]
nrow(diabetes_train)
diabetes_test     <- diabetes[-diabetes_tt_index,]
nrow(diabetes_test)

knnFit <- caret::train(Outcome ~ ., data = diabetes_train, method = "knn", tuneLength = 20)

knnFit

plot(knnFit)

apply(matrix( predict(knnFit , newdata = diabetes_test )) , 
      2, 
      postResample, 
      obs = diabetes_test$Outcome)

# Add in test set predictions
diabetes_test$estimated_prob_Outcome <-
  predict(knnFit, newdata=diabetes_test)

# Add in test set predictions
diabetes_test <- diabetes_test %>%
  mutate(pred_Outcome = relevel(factor(ifelse(estimated_prob_Outcome>0.5, "Yes", "No")),
                   ref = "No")) %>%
  mutate(Outcome_f = relevel(factor(ifelse(Outcome==1, "Yes", "No")),
                   ref = "No"))
  
# View test set probabilities
ggplot(data=diabetes_test,
       mapping=aes(x=Outcome_f, y=estimated_prob_Outcome,
                   fill=Outcome_f))+
  geom_boxplot()+
  labs(x="Outcome", y="Estimated Probability",
       title = "Estimated probability of Outcome for test set\nusing logistic regression",
       fill = "Outcome")+
  theme_classic()+
  theme(text = element_text(size=15))

caret::confusionMatrix(data = diabetes_test$pred_Outcome,
                reference = diabetes_test$Outcome_f,
                positive = "Yes")

```
The “best” tuning parameter value chosen for parameter k = 43. Root Mean Square Error (RMSE) was used as a riteria R to select this “best” parameter. In the plot, k present the Neighbors associated with the minimum RMSE value.
Accuracy = 0.7166;
Sensitivity = 0.4286;
Specificity = 0.8663;
Pos Pred Value = 0.6250;
Neg Pred Value = 0.7447

## C
Finally, we will end with using logistic regression.  We consider the same features listed in 2b with the same outcome variable.

- Train a logistic regression algorithm with all of the above features.  Print out the following results:
  - Coefficient estimate table from `summary` function (estimate, standard error, test statistic, p-value)
    - Create this table using the `tidy` function from `broom` and print out using `flextable`
  - Print out the test set performance based on Prediction Accuracy, Sensitivity, Specificity, PPV, and NPV using `flextable`.
    - **Hint**: Use `confusionMatrix` function in `caret` package.  Then convert to data frame to print as
  `flextable`

```{r 2c}
set.seed(12) # Setting seed for reproducibility

knnFit <- caret::train(Outcome ~ ., data = diabetes_train, method = "knn", tuneLength = 20)

logit_fit <- glm(formula = Outcome ~  ., data = diabetes_train, family = binomial())


logit_fit 

plot(logit_fit)

apply(matrix( predict(logit_fit  , newdata = diabetes_test )) , 
      2, 
      postResample, 
      obs = diabetes_test$Outcome)

# Add in test set predictions
diabetes_test$estimated_prob_Outcome <-
  predict(logit_fit, newdata=diabetes_test)



tidy(logit_fit) %>%
mutate(p.value =ifelse(p.value<0.005, "<0.005", as.character(round(p.value, 3))),
       estimate=as.character(round(estimate, 3)),
       std.error=as.character(round(std.error, 3)),
       statistic=as.character(round(statistic, 3)),
       term=fct_recode(factor(term),
                        "Intercept"="(Intercept)",
                        "Pregnancies"="Pregnancies", 
                        "Glucose" = "Glucose",
                        "Blood Presure" = "BloodPressure",
                        "Skin Thickness" = "SkinTickness",
                        "Insulin" = "Insulin",
                        "BMI" = "BMI",
                        "Age" = "Age" )) %>%
flextable() %>%
set_header_labels("term"="Variable",
"estimate"="Estimate",
"std.error"="Std. Error",
"statistic"="Z Statistic",
"p.value"="P-value") %>%
autofit()
                      
                      
                      
                      
# Add in test set predictions
diabetes_test <- diabetes_test %>%
  mutate(pred_Outcome = relevel(factor(ifelse(estimated_prob_Outcome>0.5, "Yes", "No")),
                   ref = "No")) %>%
  mutate(Outcome_f = relevel(factor(ifelse(Outcome==1, "Yes", "No")),
                   ref = "No"))
  

caret::confusionMatrix(data = diabetes_test$pred_Outcome,
                reference = diabetes_test$Outcome_f,
                positive = "Yes")

```
